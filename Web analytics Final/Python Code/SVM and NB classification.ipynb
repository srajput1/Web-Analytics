{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of dtm: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "size of tfidf matrix: (237, 12686)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Matrix\n",
    "with open('C:/Users/rajpu/Desktop/Web analytics Final/Data/final_data_News_Stock_quotes.csv', 'r') as f:\n",
    "    data = [tuple(line) for line in csv.reader(f)]\n",
    "    \n",
    "Lable,Date,News,Company,Sentiment,Open,High,Low,Close,Volume=zip(*data)\n",
    "\n",
    "date=list(Date)\n",
    "news=list(News)\n",
    "sent=list(Sentiment)\n",
    "lab=list(Lable)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer() \n",
    "tfidf_vect = TfidfVectorizer(stop_words=\"english\") \n",
    "\n",
    "dtm= tfidf_vect.fit_transform(news)\n",
    "\n",
    "print(\"type of dtm:\", type(dtm))\n",
    "print(\"size of tfidf matrix:\", dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of vocabulary: <class 'dict'>\n",
      "index of word 'city' in vocabulary: 2468\n",
      "total number of words: 12686\n",
      "\n",
      "Original text: \n",
      "Advertisement \n",
      "\n",
      " Eduardo Porter\n",
      " \n",
      " \n",
      "ECONOMIC SCENE\n",
      "MARCH 6, 2018\n",
      " Should Facebook pay us for our puppy pictures? Of course, the idea sounds crazy. Posting puppies on Facebook is not a chore. We love it: Facebook’s 1.4 billion daily users spend the better part of an hour on it every day. It’s amazing that we don’t have to pay for it. And yet the idea is gaining momentum in Silicon Valley and beyond: Facebook and the other technological Goliaths offering free online services — from which they harvest data from and about their users — should pay for every nugget of information they reap. The spring break pictures on Instagram, the YouTube video explaining Minecraft tactics, the internet searches and the Amazon purchases, even your speed following Waze on the way to spend Thanksgiving with your in-laws — this data is valuable. It will become more valuable, potentially much more so, in the not-too-distant future. Getting companies to pay transparently for the information will not just provide a better deal for the users whose data is scooped up as they go about their online lives. It will also improve the quality of the data on which the information economy is being built. And it could undermine the data titans’ stranglehold on technology’s future, breathing fresh air into an economy losing its vitality. Advertisement The idea has been around for a bit. Jaron Lanier, the tech philosopher and virtual-reality pioneer who now works for Microsoft Research, proposed it in his 2013 book, “Who Owns the Future?,” as a needed corrective to an online economy mostly financed by advertisers’ covert manipulation of users’ consumer choices. It is being picked up in “Radical Markets,” a book due out shortly from Eric A. Posner of the University of Chicago Law School and E. Glen Weyl, principal researcher at Microsoft. And it is playing into European efforts to collect tax revenue from American internet giants. In a report obtained last month by Politico, the European Commission proposes to impose a tax on the revenue of digital companies based on their users’ location, on the grounds that “a significant part of the value of a business is created where the users are based and data is collected and processed.” Users’ data is a valuable commodity. Facebook offers advertisers precisely targeted audiences based on user profiles. YouTube, too, uses users’ preferences to tailor its feed. Still, this pales in comparison with how valuable data is about to become, as the footprint of artificial intelligence extends across the economy. Data is the crucial ingredient of the A.I. revolution. Training systems to perform even relatively straightforward tasks like voice translation, voice transcription or image recognition requires vast amounts of data — like tagged photos, to identify their content, or recordings with transcriptions. Advertisement “Among leading A.I. teams, many can likely replicate others’ software in, at most, one to two years,” notes the technologist Andrew Ng. “But it is exceedingly difficult to get access to someone else’s data. Thus data, rather than software, is the defensible barrier for many businesses.” We may think we get a fair deal, offering our data as the price of sharing puppy pictures. By other metrics, we are being victimized: In the largest technology companies, the share of income going to labor is only about 5 to 15 percent, Mr. Posner and Mr. Weyl write. That’s way below Walmart’s 80 percent. Consumer data amounts to work they get free. “If these A.I.-driven companies represent the future of broader parts of the economy,” they argue, “without something basic changing in their business model, we may be headed for a world where labor’s share falls dramatically from its current roughly 70 percent to something closer to 20 to 30 percent.”  . As Mr. Lanier, Mr. Posner and Mr. Weyl point out, it is ironic that humans are providing free data to train the artificial-intelligence systems to replace workers across the economy. Commentators from both left and right fret over how ordinary people will put food on the table once robots take all the jobs. Perhaps a universal basic income, funded by taxes, is the answer? How about paying people for the data they produced to train the robots? If A.I. accounted for 10 percent of the economy and the big-data companies paid two-thirds of their income for data — the same as labor’s share of income across the economy — the share of income going to “workers” would rise drastically. By Mr. Weyl and Mr. Posner’s reckoning, the median household of four would gain $20,000 a year. A critical consideration is that if people were paid for their data, its quality and value would increase. Facebook could directly ask users to tag the puppy pictures to train the machines. It could ask translators to upload their translations. Facebook and Google could demand quality information if the value of the transaction were more transparent. Unwilling to enter in a direct quid pro quo with their users, the data titans must make do with whatever their users submit. The transition would not be painless. We would need to figure out systems to put value on data. Your puppy pictures might turn out to be worthless, but that college translation from Serbo-Croatian could be valuable. Barred from free data, YouTube and Facebook might charge a user fee for their service — like Netflix. Alternatively, they might make their money from training A.I. systems and pay some royalty stream to the many people whose data helped train them. But whatever the cost, the transformation seems worthwhile. Notably, it could help resolve one of the most relevant questions coming into focus in this new technological age: Who will control the data? Today, the dominant data harvesters in the business are Google and Facebook, with Amazon, Apple and Microsoft some way behind. Their dominance cannot really be challenged: Could you think of a rival search engine? Could another social network replace the one all your friends are on? This dominance might matter less if companies had to pay for their users’ data. Advertisement Google and Facebook and Amazon would not be able to extend the network effects that cemented their place at the top of the technology ecosystem to the world of A.I. Everybody wants to be on Facebook because everybody’s friends are on Facebook. But this dominance could be eroded if rivals made direct offers of money for data. Companies with different business models might join the fray. “This is an opportunity for other companies to enter and say look, we will pay you for this data,” Mr. Posner said. “All this is so new that ordinary people haven’t figured out how manipulated they are by these companies.” The big question, of course, is how we get there from here. My guess is that it would be naïve to expect Google and Facebook to start paying for user data of their own accord, even if that improved the quality of the information. Could policymakers step in, somewhat the way the European Commission did, demanding that technology companies compute the value of consumer data? In any event, there is probably a better deal out there, in your future, than giving Facebook free puppy pictures. A version of this article appears in print on March 7, 2018, on Page B1 of the New York edition with the headline: Getting Tech Giants to Pay You for Your Data.  Order Reprints| Today's Paper|Subscribe\n",
      "\n",
      " We’re interested in your feedback on this page. Tell us what you think. See More » Go to Home Page »\n",
      "\n",
      "tfidf weights: \n",
      "\n",
      "(12686,)\n",
      "[('news', 1.0), ('zwaan', 0.0), ('epidemic', 0.0), ('equifax', 0.0), ('equatorial', 0.0), ('equator', 0.0), ('equation', 0.0), ('equals', 0.0), ('equally', 0.0), ('equality', 0.0), ('episodes', 0.0), ('episode', 0.0), ('envoy', 0.0), ('equitable', 0.0), ('envisions', 0.0), ('envisioning', 0.0), ('envisioned', 0.0), ('envision', 0.0), ('environmentally', 0.0), ('environmental', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"type of vocabulary:\", type(tfidf_vect.vocabulary_))\n",
    "print(\"index of word 'city' in vocabulary:\", \\\n",
    "      tfidf_vect.vocabulary_['city'])\n",
    "voc_lookup={tfidf_vect.vocabulary_[word]:word \\\n",
    "            for word in tfidf_vect.vocabulary_}\n",
    "print(\"total number of words:\", len(voc_lookup ))\n",
    "\n",
    "print(\"\\nOriginal text: \\n\"+news[1])\n",
    "\n",
    "print(\"\\ntfidf weights: \\n\")\n",
    "doc0=dtm[0].toarray()[0]\n",
    "print(doc0.shape)\n",
    "\n",
    "\n",
    "top_words=(doc0.argsort())[::-1][0:20]\n",
    "print([(voc_lookup[i], doc0[i]) for i in top_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.86      0.84      0.85        43\n",
      "          2       0.77      0.79      0.78        29\n",
      "\n",
      "avg / total       0.82      0.82      0.82        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 3\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                dtm, sent, test_size=0.3, random_state=0)\n",
    "# train a multinomial naive Bayes model using the testing data\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted=clf.predict(X_test)\n",
    "\n",
    "labels=sorted(list(set(sent)))\n",
    "\n",
    "precision, recall, fscore, support=\\\n",
    "     precision_recall_fscore_support(\\\n",
    "     y_test, predicted, labels=labels)\n",
    "    \n",
    "#print(precision)\n",
    "#print(recall)\n",
    "#print(fscore)\n",
    "#print(support)\n",
    "\n",
    "print(classification_report(y_test, predicted, target_names=labels))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data set average precision:\n",
      "[ 0.53809524  0.82894737  0.78571429  0.77708333  0.79166667]\n",
      "\n",
      "Test data set average recall:\n",
      "[ 0.5084058   0.7173913   0.60869565  0.74094203  0.77840909]\n",
      "\n",
      "Test data set average fscore:\n",
      "[ 0.5009009   0.6998557   0.54220779  0.73496241  0.77884615]\n",
      "\n",
      " Train data set average fscore:\n",
      "[ 0.93028099  0.62586422  0.63673683  0.64026144  0.615519  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "metrics = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "\n",
    "clf = MultinomialNB()\n",
    "#clf = MultinomialNB(alpha=0.8)\n",
    "\n",
    "cv = cross_validate(clf, dtm, sent, scoring=metrics, cv=5)\n",
    "print(\"Test data set average precision:\")\n",
    "print(cv['test_precision_macro'])\n",
    "print(\"\\nTest data set average recall:\")\n",
    "print(cv['test_recall_macro'])\n",
    "print(\"\\nTest data set average fscore:\")\n",
    "print(cv['test_f1_macro'])\n",
    "\n",
    "# To see the performance of training data set use \n",
    "#cv['train_xx_macro']\n",
    "print(\"\\n Train data set average fscore:\")\n",
    "print(cv['train_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data set average precision:\n",
      "[ 0.57166667  0.77097902  0.74456522  0.82358871  0.87884615]\n",
      "\n",
      "Test data set average recall:\n",
      "[ 0.58434783  0.76956522  0.74456522  0.79076087  0.87310606]\n",
      "\n",
      "Test data set average fscore:\n",
      "[ 0.57738095  0.76993464  0.74456522  0.78240741  0.86931818]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#SVM model\n",
    "\n",
    "metrics = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "\n",
    "# initiate an linear SVM model\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "cv = cross_validate(clf, dtm, sent, scoring=metrics, cv=5)\n",
    "print(\"Test data set average precision:\")\n",
    "print(cv['test_precision_macro'])\n",
    "print(\"\\nTest data set average recall:\")\n",
    "print(cv['test_recall_macro'])\n",
    "print(\"\\nTest data set average fscore:\")\n",
    "print(cv['test_f1_macro'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.87      0.79      0.83        43\n",
      "          2       0.73      0.83      0.77        29\n",
      "\n",
      "avg / total       0.81      0.81      0.81        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 3\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                dtm, sent, test_size=0.3, random_state=0)\n",
    "# train a multinomial naive Bayes model using the testing data\n",
    "#clf = MultinomialNB().fit(X_train, y_train)\n",
    "clf = svm.LinearSVC().fit(X_train, y_train)\n",
    "predicted=clf.predict(X_test)\n",
    "\n",
    "labels=sorted(list(set(sent)))\n",
    "\n",
    "precision, recall, fscore, support=\\\n",
    "     precision_recall_fscore_support(\\\n",
    "     y_test, predicted, labels=labels)\n",
    "    \n",
    "#print(precision)\n",
    "#print(recall)\n",
    "#print(fscore)\n",
    "#print(support)\n",
    "\n",
    "print(classification_report(y_test, predicted, target_names=labels))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
