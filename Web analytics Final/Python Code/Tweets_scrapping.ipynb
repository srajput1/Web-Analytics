{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import datetime\n",
    "from datetime import date\n",
    "from pandas_datareader.data import DataReader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "consumer_key = 'Nt9OPpV3YTsRkFlUdzRclki6Y'\n",
    "consumer_secret = 'qonyyVaw6ISlHa1cHrQoiug9iBEJrjqQfv2J6tveaPSyycapxl'\n",
    "access_token = '995853085-85KRxkGxhdBPkF9cTs4oEJS6uWhxMj4ZZTndv9M0'\n",
    "access_secret = '41R90OY0tHa7isHU9ozdLSS7OXNjojl3x28DpCr5GC1Kv'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "class MyListener(StreamListener):\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, output_file, time_limit):\n",
    "        \n",
    "            # attribute to get listener start time\n",
    "            self.start_time=datetime.datetime.now()\n",
    "            \n",
    "            # attribute to set time limit for listening\n",
    "            self.time_limit=time_limit\n",
    "            \n",
    "            # attribute to set the output file\n",
    "            self.output_file=output_file\n",
    "            \n",
    "            # initiate superclass's constructor\n",
    "            StreamListener.__init__(self)\n",
    "    \n",
    "    # on_data is invoked when a tweet comes in\n",
    "    # overwrite this method inheritted from superclass\n",
    "    # when a tweet comes in, the tweet is passed as \"data\"\n",
    "    def on_data(self, data):\n",
    "        \n",
    "        # get running time\n",
    "        running_time=datetime.datetime.now()-self.start_time\n",
    "        print(running_time)\n",
    "        \n",
    "        # check if running time is over time_limit\n",
    "        if running_time.seconds/60.0<self.time_limit:\n",
    "            \n",
    "            # ***Exception handling*** \n",
    "            # If an error is encountered, \n",
    "            # a try block code execution is stopped and transferred\n",
    "            # down to the except block. \n",
    "            # If there is no error, \"except\" block is ignored\n",
    "            try:\n",
    "                # open file in \"append\" mode\n",
    "                with open(self.output_file, 'a') as f:\n",
    "                    # Write tweet string (in JSON format) into a file\n",
    "                    f.write(data)\n",
    "                    \n",
    "                    # continue listening\n",
    "                    return True\n",
    "                \n",
    "            # if an error is encountered\n",
    "            # print out the error message and continue listening\n",
    "            \n",
    "            except BaseException as e:\n",
    "                print(\"Error on_data:\" , str(e))\n",
    "                \n",
    "                # if return \"True\", the listener continues\n",
    "                return True\n",
    "            \n",
    "        else:  # timeout, return False to stop the listener\n",
    "            print(\"time out\")\n",
    "            return False\n",
    " \n",
    "    # on_error is invoked if there is anything wrong with the listener\n",
    "    # error status is passed to this method\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        # continue listening by \"return True\"\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.851490\n",
      "0:00:00.987428\n",
      "0:00:01.531245\n",
      "0:00:01.731668\n",
      "0:00:01.929762\n",
      "0:00:02.054202\n",
      "0:00:02.272696\n",
      "0:00:02.696057\n",
      "0:00:02.944054\n",
      "0:00:03.306004\n",
      "0:00:04.430777\n",
      "0:00:05.691748\n",
      "0:00:07.156451\n",
      "0:00:07.881499\n",
      "0:00:08.872725\n",
      "0:00:09.303225\n",
      "0:00:09.345269\n",
      "0:00:09.525132\n",
      "0:00:10.202690\n",
      "0:00:10.437762\n",
      "0:00:10.922978\n",
      "0:00:11.480170\n",
      "0:00:11.731050\n",
      "0:00:11.785200\n",
      "0:00:11.936866\n",
      "0:00:12.727512\n",
      "0:00:13.860759\n",
      "0:00:14.044973\n",
      "0:00:14.929131\n",
      "0:00:15.217154\n",
      "0:00:15.847969\n",
      "0:00:15.931340\n",
      "0:00:16.356641\n",
      "0:00:16.448626\n",
      "0:00:17.567457\n",
      "0:00:17.975773\n",
      "0:00:18.263290\n",
      "0:00:19.179721\n",
      "0:00:19.433354\n",
      "0:00:19.729903\n",
      "0:00:20.800437\n",
      "0:00:21.058905\n",
      "0:00:21.528395\n",
      "0:00:21.740256\n",
      "0:00:22.448298\n",
      "0:00:22.455410\n",
      "0:00:22.586756\n",
      "0:00:23.004007\n",
      "0:00:23.028037\n",
      "0:00:23.276948\n",
      "0:00:23.582880\n",
      "0:00:24.246194\n",
      "0:00:24.547675\n",
      "0:00:25.538649\n",
      "0:00:25.590717\n",
      "0:00:25.921711\n",
      "0:00:26.263409\n",
      "0:00:26.297530\n",
      "0:00:26.704906\n",
      "0:00:27.354359\n",
      "0:00:27.394297\n",
      "0:00:28.114071\n",
      "0:00:28.202188\n",
      "0:00:28.496030\n",
      "0:00:29.011872\n",
      "0:00:29.152640\n",
      "0:00:29.315371\n",
      "0:00:29.865515\n",
      "0:00:29.997691\n",
      "0:00:30.140355\n",
      "0:00:31.317921\n",
      "0:00:31.472659\n",
      "0:00:31.880188\n",
      "0:00:32.785426\n",
      "0:00:32.946809\n",
      "0:00:33.613266\n",
      "0:00:33.954481\n",
      "0:00:34.305585\n",
      "0:00:34.632689\n",
      "0:00:35.951048\n",
      "0:00:36.351497\n",
      "0:00:36.555752\n",
      "0:00:38.040040\n",
      "0:00:38.681736\n",
      "0:00:39.248494\n",
      "0:00:40.364567\n",
      "0:00:40.642300\n",
      "0:00:41.015862\n",
      "0:00:41.228654\n",
      "0:00:41.428920\n",
      "0:00:42.312296\n",
      "0:00:42.737540\n",
      "0:00:43.586428\n",
      "0:00:44.886873\n",
      "0:00:45.295329\n",
      "0:00:45.695608\n",
      "0:00:46.051966\n",
      "0:00:46.203574\n",
      "0:00:46.606302\n",
      "0:00:47.021153\n",
      "0:00:48.510441\n",
      "0:00:48.634635\n",
      "0:00:48.863596\n",
      "0:00:51.158553\n",
      "0:00:51.332263\n",
      "0:00:51.455464\n",
      "0:00:51.784224\n",
      "0:00:51.932484\n",
      "0:00:52.028585\n",
      "0:00:52.101096\n",
      "0:00:52.165369\n",
      "0:00:52.511866\n",
      "0:00:52.690450\n",
      "0:00:54.186150\n",
      "0:00:55.078554\n",
      "0:00:55.694783\n",
      "0:00:56.284833\n",
      "0:00:56.345024\n",
      "0:00:56.670746\n",
      "0:00:57.018852\n",
      "0:00:57.227779\n",
      "0:00:57.500323\n",
      "0:00:57.820920\n",
      "0:00:58.511131\n",
      "0:00:59.259699\n",
      "0:00:59.340260\n",
      "0:01:00.040404\n",
      "time out\n"
     ]
    }
   ],
   "source": [
    "# initiate an instance of MyListener \n",
    "tweet_listener=MyListener(output_file=\"python.csv\",time_limit=10)\n",
    "\n",
    "# start a staeam instance using authentication and the listener\n",
    "twitter_stream = Stream(auth, tweet_listener)\n",
    "# filtering tweets by topics\n",
    "twitter_stream.filter(track=['#google', '#bofa','#bankofamerica','#netflix','#facebook','#apple','#microsoft','#boing','#amazon','#amazon','#jpmorgan',\"#yahoo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#google','#stocks'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1),fromDate=20180301,toDate=201830310)\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('google_bofa.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#bankofamerica'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_bankofamerica.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#bofa'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_bofa.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#netflix'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_netflix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#fb'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_fb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#apple'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_apple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#microsoft'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_microsoft.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#boing'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_boing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#amazon','#stocks'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#jpmorgan'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_jpmorgan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searched_tweets = []\n",
    "tweets=[]\n",
    "max_tweets=1000\n",
    "last_id = -1\n",
    "\n",
    "query='#yahoo'\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "while len(searched_tweets) < max_tweets:\n",
    "    count = max_tweets - len(searched_tweets)\n",
    "    try:\n",
    "        # for each search, at maximum you get 100 results, although\n",
    "        # you can set count larger than 100\n",
    "        # You can limit the id for the most recent tweets (max_id)\n",
    "        # query can be a list of hashtags\n",
    "        # search api returns tweets sorted by time in descending order\n",
    "        new_tweets = api.search(q=query, count=count, max_id=str(last_id - 1))\n",
    "\n",
    "        if not new_tweets:\n",
    "            break\n",
    "        # append new batch into list    \n",
    "        searched_tweets.extend(new_tweets)\n",
    "        # only store a list of (date, tweet text) \n",
    "        tweets+=[(item.created_at, item.text) for item in new_tweets]\n",
    "        \n",
    "        # get the first tweet in the batch\n",
    "        last_id = new_tweets[-1].id\n",
    "\n",
    "    except tweepy.TweepError as e:\n",
    "        # depending on TweepError.code, one may want to retry or wait\n",
    "        # to keep things simple, we will give up on an error\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(tweets)\n",
    "#print(my_df)\n",
    "my_df.to_csv('python_yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
