{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get_attribute'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-73e3a056b750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m#data_div = driver.find_elements_by_class_name('SearchResults-main--3t9sI')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mdata_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'innerHTML'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mdata_html\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'innerHTML'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0msoup1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_html\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html5lib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get_attribute'"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def fetching_links(soup):\n",
    "    \n",
    "    soup1 = soup\n",
    "    a_tag = soup1.findAll('a')\n",
    "\n",
    "    str_a_tag = []\n",
    "    #converting each entry to string for ease of operation\n",
    "    for a in a_tag:\n",
    "        str_a_tag.append(str(a))\n",
    "\n",
    "    #reomving href\n",
    "    m = [(a[9:]) for a in str_a_tag]\n",
    "\n",
    "    #split the resulting string using \" to get the link\n",
    "    s = [a.split('\"') for a in m]\n",
    "    #appending all the links in a new list\n",
    "    links=[]\n",
    "    for i in range(len(s)):\n",
    "        links.append(s[i][0])\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_data(url):\n",
    "    \n",
    "\n",
    "    #for link in links:\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\Users\\\\rajpu\\\\Desktop\\\\BIA 660\\\\chromedriver_win32\\\\chromedriver.exe',\\\n",
    "                          chrome_options=chrome_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    page_link = requests.get(url)\n",
    "    time.sleep(10)\n",
    "    soup_link = BeautifulSoup(page_link.content, 'html.parser')\n",
    "    data = soup_link.findAll('p')\n",
    "    time.sleep(10)\n",
    "    driver.close()\n",
    "    news_artilce=[]\n",
    "    news_artilce_str = []\n",
    "    for a in data:\n",
    "        news_artilce.append(a.text)\n",
    "    \n",
    "    #news_artilce.remove(\"12 Please verify you're not a robot by clicking the box.\" or \"Invalid email address. Please re-enter.\"\\\n",
    "    #                    or \"You must select a newsletter to subscribe to.\" or \"View all New York Times newsletters\")\n",
    "    news_article_str = \" \".join(news_artilce[0:])\n",
    "    news_article_str1 = news_article_str.replace(\"Please verify you're not a robot by clicking the box. Invalid email address. Please re-enter. You must select a newsletter to subscribe to. View all New York Times newsletters.\",\".\")\n",
    "    #print(news_artilce)\n",
    "    #print(news_article_str)\n",
    "    return news_article_str1\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\n",
    "\n",
    "    url = ['https://query.nytimes.com/search/sitesearch/#/amazon/1days/allresults/1/allauthors/relevance/Business/']\n",
    "      #      'https://query.nytimes.com/search/sitesearch/#/microsoft/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #     'https://query.nytimes.com/search/sitesearch/#/Bank of America/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #    'https://query.nytimes.com/search/sitesearch/#/Netflix/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #     'https://query.nytimes.com/search/sitesearch/#/Yahoo/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #     'https://query.nytimes.com/search/sitesearch/#/Google/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #     'https://query.nytimes.com/search/sitesearch/#/JP Morgan/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #     'https://query.nytimes.com/search/sitesearch/#/Apple/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #     'https://query.nytimes.com/search/sitesearch/#/Boeing/30days/allresults/1/allauthors/relevance/Business/'\\\n",
    "      #     'https://query.nytimes.com/search/sitesearch/#/Facebook/30days/allresults/1/allauthors/relevance/Business/']        \n",
    "\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    driver = webdriver.Chrome(executable_path ='C:\\\\Users\\\\rajpu\\\\Desktop\\\\BIA 660\\\\chromedriver_win32\\\\chromedriver.exe',\\\n",
    "                              chrome_options=chrome_options)\n",
    "    #url = ['https://query.nytimes.com/search/sitesearch/#/google/7days/allresults/1/allauthors/relevance/Business/']\n",
    "\n",
    "    for a in range(len(url)):\n",
    "        driver.get(url[a])\n",
    "        page = requests.get(url[a])\n",
    "\n",
    "    #creating a list to get links    \n",
    "    links = []\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    #data_div = driver.find_elements_by_class_name('SearchResults-main--3t9sI')\n",
    "    data_html = data_div.get_attribute('innerHTML')\n",
    "    data_html=data_div.get_data('innerHTML')\n",
    "    soup1 = BeautifulSoup(data_html, 'html5lib')\n",
    "\n",
    "    links.append(fetching_links(soup1))\n",
    "\n",
    "\n",
    "    try:\n",
    "        next_new = driver.find_element_by_class_name('next')\n",
    "\n",
    "        while  next_new != \"\":\n",
    "            next_new = driver.find_element_by_class_name('next')\n",
    "            next_new.click()\n",
    "            url_next = driver.current_url\n",
    "            #print(url_next)\n",
    "            page = requests.get(url_next)\n",
    "            time.sleep(10)\n",
    "            soup2 = BeautifulSoup(page.content, 'html.parser')\n",
    "            data_div = driver.find_elements_by_class_name('SearchResults-main--3t9sI')\n",
    "            data_html = data_div.get_attribute('innerHTML')\n",
    "            soup3 = BeautifulSoup(data_html, 'html5lib')\n",
    "            time.sleep(10)\n",
    "            links.append(fetching_links(soup3))\n",
    "            time.sleep(30)\n",
    "    except NoSuchElementException: \n",
    "        pass\n",
    "    driver.close()\n",
    "    print(links)\n",
    "    links_final = []\n",
    "    for link in links:\n",
    "        for l1 in link:\n",
    "            if l1 not in links_final:\n",
    "                links_final.append(l1)\n",
    "    print(links_final)\n",
    "\n",
    "    #getting the news articles in this list\n",
    "\n",
    "    news = []    \n",
    "    for link in links_final:\n",
    "        print(link)\n",
    "        news.append(get_data(link))\n",
    "        print(news)\n",
    "    #csvfile = 'C:/Users/shrey/OneDrive/Desktop/seaktop/660 Web/Project/News.csv'\n",
    "    #with open(csvfile, \"w\") as output:\n",
    "    #    writer = csv.writer(output, lineterminator='\\n')\n",
    "    #    for val in news:\n",
    "    #        writer.writerow([val]) \n",
    "    #print(news)\n",
    "\n",
    "    my_df = pd.DataFrame(news)\n",
    "    print(my_df)\n",
    "    my_df.to_csv ('C:/Users/rajpu/Desktop/BIA 660/news_amazon.csv')\n",
    "\n",
    "\n",
    "    #print(url[0])\n",
    "    #start_chrome(url[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
