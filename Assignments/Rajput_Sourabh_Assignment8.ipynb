{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              NUM_DENSE_UNITS=100,\\\n",
    "              # number of units in dense layer\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.0):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    \n",
    "    if len(conv_blocks)>1:\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "    else:\n",
    "        z=conv_blocks[0]\n",
    "        \n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(NUM_DENSE_UNITS, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk,string\n",
    "\n",
    "\n",
    "data=pd.read_csv('train.csv',encoding='Latin1', header=None)\n",
    "trainDataReview=data[0]\n",
    "trainDataSentiment=data[1]\n",
    "del trainDataSentiment[0]\n",
    "del trainDataReview[0]\n",
    "#data\n",
    "data1=pd.read_csv('test.csv',encoding='Latin1', header=None)\n",
    "#data1\n",
    "testdata1_Review=data1[0]\n",
    "testdata1_Sentiment=data1[1]\n",
    "del testdata1_Review[0]\n",
    "del testdata1_Sentiment[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.72000, saving model to best_model\n",
      "38s - loss: 0.6912 - acc: 0.5120 - val_loss: 0.6757 - val_acc: 0.7200\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc did not improve\n",
      "38s - loss: 0.6269 - acc: 0.6850 - val_loss: 0.6196 - val_acc: 0.6850\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.72000 to 0.76500, saving model to best_model\n",
      "37s - loss: 0.4680 - acc: 0.8190 - val_loss: 0.5035 - val_acc: 0.7650\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc improved from 0.76500 to 0.77000, saving model to best_model\n",
      "37s - loss: 0.2650 - acc: 0.9120 - val_loss: 0.4629 - val_acc: 0.7700\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.77000 to 0.81500, saving model to best_model\n",
      "38s - loss: 0.1271 - acc: 0.9630 - val_loss: 0.4251 - val_acc: 0.8150\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "38s - loss: 0.0650 - acc: 0.9850 - val_loss: 0.3905 - val_acc: 0.8050\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc did not improve\n",
      "38s - loss: 0.0470 - acc: 0.9870 - val_loss: 0.4721 - val_acc: 0.7850\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc improved from 0.81500 to 0.82000, saving model to best_model\n",
      "38s - loss: 0.0284 - acc: 0.9930 - val_loss: 0.4115 - val_acc: 0.8200\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "# set the maximum number of words to be used\n",
    "MAX_NB_WORDS=10000\n",
    "\n",
    "# set sentence/document length\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "# get a Keras tokenizer\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(trainDataReview)\n",
    "\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(trainDataReview)\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_seq = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(testdata1_Review)\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_test_seq = pad_sequences(test_seq, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=1\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "BTACH_SIZE = 32\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "BEST_MODEL_FILEPATH='best_model'\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# Assume we only have 500 labeled data\n",
    "# split dataset into train (70%) and test sets (20%)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, \\\n",
    "                EMBEDDING_DIM=300,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_FILTERS=num_filters,\\\n",
    "                NUM_DENSE_UNITS=dense_units_num)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=1, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(padded_seq, trainDataSentiment, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[padded_test_seq, testdata1_Sentiment], verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.67569261550903326, 0.61960777759551999, 0.50349781990051268, 0.4629493045806885, 0.4251461398601532, 0.39050834774971011, 0.47210723161697388, 0.41147908449172976], 'val_acc': [0.71999999999999997, 0.68500000000000005, 0.76500000000000001, 0.77000000000000002, 0.81499999999999995, 0.80500000000000005, 0.78500000000000003, 0.81999999999999995], 'loss': [0.6911957511901855, 0.62689066886901856, 0.4679721283912659, 0.26501093983650209, 0.12712891827523709, 0.065046991229057313, 0.046971832394599913, 0.028428204238414766], 'acc': [0.51200000000000001, 0.68500000000000005, 0.81899999999999995, 0.91200000000000003, 0.96299999999999997, 0.98499999999999999, 0.98699999999999999, 0.99299999999999999]}\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "acc: 82.00%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.84      0.82        99\n",
      "          1       0.84      0.80      0.82       101\n",
      "\n",
      "avg / total       0.82      0.82      0.82       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(training.history)\n",
    "\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "# predict\n",
    "pred=model.predict(padded_test_seq)\n",
    "pred=np.where(pred>0.5,1,0)\n",
    "print(pred[0:5])\n",
    "# evaluate the model\n",
    "scores = model.evaluate(padded_test_seq, testdata1_Sentiment, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "setimentTestNum=pd.to_numeric(testdata1_Sentiment)\n",
    "setimentTrainNum=pd.to_numeric(trainDataSentiment)\n",
    "print(classification_report(setimentTestNum, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajpu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS=12000\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "# tokenizer.word_index provides the mapping \n",
    "# between a word and word index for all words\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "\n",
    "# \"+1\" is for padding symbol\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "ignored_words=[]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # if word_index is above the max number of words, ignore it\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]\n",
    "    else:\n",
    "        ignored_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the maximum number of words to be used\n",
    "MAX_NB_WORDS=12000\n",
    "\n",
    "# set sentence/document length\n",
    "MAX_DOC_LEN=1000\n",
    "\n",
    "# get a Keras tokenizer\n",
    "# https://keras.io/preprocessing/text/\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(trainDataReview)\n",
    "\n",
    "# convert each document to a list of word index as a sequence\n",
    "sequences = tokenizer.texts_to_sequences(trainDataReview)\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(testdata1_Review)\n",
    "\n",
    "# pad all sequences into the same length \n",
    "# if a sentence is longer than maxlen, pad it in the right\n",
    "# if a sentence is shorter than maxlen, truncate it in the right\n",
    "padded_test_sequences = pad_sequences(test_sequences, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.53000, saving model to best_model\n",
      "27s - loss: 0.7347 - acc: 0.4990 - val_loss: 0.6805 - val_acc: 0.5300\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.53000 to 0.70500, saving model to best_model\n",
      "27s - loss: 0.6453 - acc: 0.6320 - val_loss: 0.6311 - val_acc: 0.7050\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.70500 to 0.77000, saving model to best_model\n",
      "26s - loss: 0.5901 - acc: 0.6870 - val_loss: 0.5566 - val_acc: 0.7700\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc improved from 0.77000 to 0.79000, saving model to best_model\n",
      "26s - loss: 0.4721 - acc: 0.7950 - val_loss: 0.4700 - val_acc: 0.7900\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc did not improve\n",
      "26s - loss: 0.3927 - acc: 0.8230 - val_loss: 0.4425 - val_acc: 0.7900\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "27s - loss: 0.3016 - acc: 0.8680 - val_loss: 0.4473 - val_acc: 0.7900\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc improved from 0.79000 to 0.80000, saving model to best_model\n",
      "26s - loss: 0.2351 - acc: 0.9130 - val_loss: 0.3919 - val_acc: 0.8000\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc improved from 0.80000 to 0.82000, saving model to best_model\n",
      "27s - loss: 0.1558 - acc: 0.9510 - val_loss: 0.3630 - val_acc: 0.8200\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.82000 to 0.82500, saving model to best_model\n",
      "27s - loss: 0.1120 - acc: 0.9660 - val_loss: 0.3807 - val_acc: 0.8250\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc did not improve\n",
      "27s - loss: 0.0792 - acc: 0.9810 - val_loss: 0.4000 - val_acc: 0.8250\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "26s - loss: 0.0637 - acc: 0.9830 - val_loss: 0.4089 - val_acc: 0.8050\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc improved from 0.82500 to 0.83000, saving model to best_model\n",
      "27s - loss: 0.0417 - acc: 0.9900 - val_loss: 0.3836 - val_acc: 0.8300\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc did not improve\n",
      "26s - loss: 0.0356 - acc: 0.9930 - val_loss: 0.4211 - val_acc: 0.8150\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc did not improve\n",
      "27s - loss: 0.0389 - acc: 0.9900 - val_loss: 0.3909 - val_acc: 0.8300\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc did not improve\n",
      "27s - loss: 0.0247 - acc: 0.9960 - val_loss: 0.4597 - val_acc: 0.8100\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "26s - loss: 0.0241 - acc: 0.9940 - val_loss: 0.4331 - val_acc: 0.8200\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EMBEDDING_DIM=300\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "output_units_num=1\n",
    "num_filters=64\n",
    "\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "BTACH_SIZE = 32\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# Assume we only have 500 labeled data\n",
    "# split dataset into train (70%) and test sets (20%)\n",
    "\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, \\\n",
    "                EMBEDDING_DIM=300,\\\n",
    "                NUM_OUTPUT_UNITS=output_units_num, \\\n",
    "                NUM_FILTERS=num_filters,\\\n",
    "                NUM_DENSE_UNITS=dense_units_num,\\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_acc', patience=3, verbose=2, mode='max')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(padded_sequences, trainDataSentiment, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[padded_test_sequences, testdata1_Sentiment], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.68049312829971309, 0.63106023550033574, 0.55663531303405767, 0.47004890680313111, 0.44251871466636655, 0.44729318380355837, 0.39193913459777829, 0.36304752171039584, 0.38067011952400209, 0.40003479480743409, 0.40885510921478274, 0.38361028075218201, 0.42110904097557067, 0.39088418245315554, 0.45965344905853273, 0.43311402678489686], 'val_acc': [0.53000000000000003, 0.70499999999999996, 0.77000000000000002, 0.79000000000000004, 0.79000000000000004, 0.79000000000000004, 0.80000000000000004, 0.81999999999999995, 0.82499999999999996, 0.82499999999999996, 0.80500000000000005, 0.82999999999999996, 0.81499999999999995, 0.82999999999999996, 0.81000000000000005, 0.81999999999999995], 'loss': [0.73471386241912839, 0.64533096146583557, 0.59013731098175048, 0.47205068016052248, 0.3926629219055176, 0.30155634641647339, 0.23514338469505311, 0.15578502854704857, 0.11201140630245209, 0.079159229844808582, 0.063684763222932816, 0.041681142210960385, 0.035580520361661913, 0.038918213441967962, 0.024712588682770729, 0.024125574607402087], 'acc': [0.499, 0.63200000000000001, 0.68700000000000006, 0.79500000000000004, 0.82299999999999995, 0.86799999999999999, 0.91300000000000003, 0.95099999999999996, 0.96599999999999997, 0.98099999999999998, 0.98299999999999998, 0.98999999999999999, 0.99299999999999999, 0.98999999999999999, 0.996, 0.99399999999999999]}\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "acc: 83.00%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.84      0.83        99\n",
      "          1       0.84      0.82      0.83       101\n",
      "\n",
      "avg / total       0.83      0.83      0.83       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(training.history)\n",
    "\n",
    "model.load_weights(\"best_model\")\n",
    "\n",
    "# predict\n",
    "pred=model.predict(padded_test_seq)\n",
    "pred=np.where(pred>0.5,1,0)\n",
    "print(pred[0:5])\n",
    "# evaluate the model\n",
    "scores = model.evaluate(padded_test_seq, testdata1_Sentiment, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "setimentTestNum=pd.to_numeric(testdata1_Sentiment)\n",
    "setimentTrainNum=pd.to_numeric(trainDataSentiment)\n",
    "print(classification_report(setimentTestNum, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
