{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "[]\n",
      "\n",
      "sentiment\n",
      "2\n",
      "\n",
      "accuracy\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "#Sourabh Rajput\n",
    "#BIA 660 C\n",
    "#CWID 10431188\n",
    "\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text=text.lower() #converts to lower case\n",
    "    t1 = nltk.word_tokenize(text) #tokenizes the lowercased string into tokens\n",
    "    tokens=[] \n",
    "    tokens1=[]         \n",
    "     \n",
    "    s=[tokens for tokens in t1 if re.search(r'^[^-_].[^0-9].*[^_-]$.[^A-Z a-z]', tokens)] #token only contains letters (i.e. a-z or A-Z), \"-\" , or \"_\" . A token cannot starts or ends with \"-\" or \"_\" . \n",
    "    #print(s)\n",
    "    for t in s:\n",
    "        #t.strip()\n",
    "        if len(t)>1:\n",
    "            tokens1.append(t) #only those tokens are added which has more than one characters\n",
    "    \n",
    "    #print (tokens1)\n",
    "    vocabulary= set(tokens1) \n",
    "    #print (vocabulary)\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    #print (stop_words)\n",
    "   \n",
    "    for token in vocabulary:  #removes stop words and added to tokens\n",
    "          if token not in stop_words:\n",
    "                tokens.append(token)  \n",
    "           \n",
    "    \n",
    "     \n",
    "    return tokens # returns the resulting listing token as output\n",
    "\n",
    "\n",
    "def sentiment_analysis(text, positive_words, negative_words):\n",
    "    \n",
    "    sentiment=None\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text) #tokenizes the lowercased string into tokens    \n",
    "    positive_tokens=[]\n",
    "    negative_tokens=[]\n",
    "    #negations=['not', 'n\\'t',' no', 'cannot', 'neither', 'nor', 'too']\n",
    "    \n",
    "    positive_list=[token for token in tokens if token in positive_words] #counts positive words in each list\n",
    "    #print(len(positive_list))\n",
    "    \n",
    "    negative_list=[token for token in tokens if token in negative_words] #counts negative words in each list\n",
    "    #print(len(negative_list))\n",
    "    \n",
    "    for idx, token in enumerate(positive_list):\n",
    "        if token in positive_words:\n",
    "            if idx>0:\n",
    "                if tokens[idx-1] not in negative_words: #a positive word not preceded by a negation word\n",
    "                    positive_tokens.append(token)\n",
    "                if tokens[idx-1]  in negative_words: #a negative word preceded by a negation word\n",
    "                    negative_tokens.append(token)\n",
    "                \n",
    "            else:\n",
    "                positive_tokens.append(token)\n",
    "    #print(len(positive_tokens))\n",
    "                \n",
    "    for idx, token in enumerate(negative_list):\n",
    "        if token in negative_words:\n",
    "            if idx>0:\n",
    "                if tokens[idx-1] not in negative_words: #a negative word not preceded by a negation word\n",
    "                    negative_tokens.append(token)\n",
    "                if tokens[idx-1]  in negative_words:# a positive word preceded by a negation word\n",
    "                    positive_tokens.append(token)\n",
    "            else:\n",
    "                negative_tokens.append(token)\n",
    "                \n",
    "    #print(len(negative_tokens))\n",
    "\n",
    "    if len(positive_tokens)>len(negative_tokens): # compares count of positive and negative word count\n",
    "        sentiment = 2 # When positive word count is more then it will print statement 2\n",
    "    else:\n",
    "        sentiment = 1 #When negative word count is more then it will print statement 1\n",
    "    \n",
    "    return sentiment #returns the sentiment\n",
    "\n",
    "def performance_evaluate(input_file, positive_words, negative_words):\n",
    "    \n",
    "    accuracy=None\n",
    "    count = 0 #initialize counter with zero\n",
    "    \n",
    "    with open(input_file,'r') as f: #takes an input file (\"amazon_review_300.csv\"), a list of positive words, and a list of negative words as inputs. \n",
    "        data = [line for line in csv.reader(f)]  #The input file has a list of reviews in theformat of (label, title, review). Use label (either '2' or '1') and review columns (i.e. columns 1 and 3 only) here.\n",
    "        \n",
    "    #print(data)\n",
    "    \n",
    "    new=[]\n",
    "    mew=[]\n",
    "\n",
    "    for review,title,lable in data:# makes rewiew list \n",
    "        new.append(review)\n",
    "        mew.append(lable)\n",
    "\n",
    "    count_data = zip(new,mew)\n",
    "    demo_review = list(count_data)\n",
    "    for label, review in demo_review: #makes labe list \n",
    "        result = str(sentiment_analysis(review,positive_words, negative_words))\n",
    "    \n",
    "        if result == label: #counts\n",
    "            count+=1  \n",
    "\n",
    "    accuracy = count/len(demo_review) #calculating result in accuracy\n",
    "    return accuracy #returns the accuracy as the number of correct sentiment predictions/total reviews\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    text=\"this is a  breath-taking  ambitious movie; test  text: abc_dcd abc_ dvr89w,ads34 abc-dcd -abc \"\n",
    "\n",
    "    tokens=tokenize(text)\n",
    "    print(\"tokens:\")\n",
    "    print(tokens)\n",
    "    \n",
    "    with open(\"positive-words.txt\",'r') as f:\n",
    "        positive_words=[line.strip() for line in f]\n",
    "        \n",
    "    with open(\"negative-words.txt\",'r') as f:\n",
    "        negative_words=[line.strip() for line in f]\n",
    "        #print(negative_words)\n",
    "    print(\"\\nsentiment\")\n",
    "    sentiment=sentiment_analysis(text, positive_words, negative_words)\n",
    "    print(sentiment)\n",
    "    \n",
    "   \n",
    "    accuracy=performance_evaluate(\"amazon_review_300.csv\", positive_words, negative_words)\n",
    "    \n",
    "    print(\"\\naccuracy\")\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
